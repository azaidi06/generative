[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Random forays into generative modeling. Starting with basic models/items like autoencoders and slowly making the way up to more complex things like stable diffusion(?)\nFor the most part everything will be done with Pytorch/FastAI"
  },
  {
    "objectID": "vae.html",
    "href": "vae.html",
    "title": "Variational Autoencoders",
    "section": "",
    "text": "Autoencoders are pretty neat, but can we improve our models ability to understand the multi-dimensional space of our data?\n\nIn this notebook we’ll play with variational autoencoders to get a better feel for how they work and understand how they improve upon our vanilla autoencoder\nWe’ll add some dimensions to our embedding space and see how we can sample from that differently from our normal autoencoder\nAdd explanation of VAE(!)********"
  },
  {
    "objectID": "vae.html#dataset",
    "href": "vae.html#dataset",
    "title": "Variational Autoencoders",
    "section": "Dataset",
    "text": "Dataset\n\nOur dataset is MNIST\n\nA selection of ~70,000 handwritten digits – a few sampels can be seen below:\n\n\n\n\nCode\nplot_dataset(train_df, num_samples=3)\n\n\n\n\n\n\n\nCode\n##| echo: false\n##| output: false\nlearner.fine_tune(12, 3e-2)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n5517.600586\n5498.638184\n00:09\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n4891.000488\n5152.509766\n00:09\n\n\n1\n4193.120605\n4459.402344\n00:09\n\n\n2\n3492.046143\n3789.639160\n00:09\n\n\n3\n2972.184326\ninf\n00:09\n\n\n4\n2806.825684\n3257.438721\n00:09\n\n\n5\n2768.563477\n9286.187500\n00:09\n\n\n6\n2725.435791\n3127.360596\n00:09\n\n\n7\n2669.043701\n3085.611572\n00:09\n\n\n8\n2644.336670\n3081.457764\n00:09\n\n\n9\n2591.880371\n3039.255615\n00:09\n\n\n10\n2564.253906\n3029.602295\n00:09\n\n\n11\n2568.311523\n3029.368896\n00:09"
  },
  {
    "objectID": "vae.html#reconstructed-images",
    "href": "vae.html#reconstructed-images",
    "title": "Variational Autoencoders",
    "section": "Reconstructed Images",
    "text": "Reconstructed Images\n\n\n\n\n\n\nStill pretty good, but we were more interested in being able to sample from the embedding space!"
  },
  {
    "objectID": "vae.html#latent-space-exploration",
    "href": "vae.html#latent-space-exploration",
    "title": "Variational Autoencoders",
    "section": "Latent space exploration",
    "text": "Latent space exploration\n\nLets see how our validation set gets mapped into embedding space via our encoder\n\nReduced overlap because we can project into 3 dimensions\nEven for points outside the distribution that it’s already seen, it is able to map them into something meaningful\n\n\n\n\nCode\nplot_latent_regen(model, min=-4, max=4)\n\n\n\n\n\nleft: embedding scatter plot  right: reconstructed latents\n\n\n\nFigure 2: Embedding Space and Latent Reconstructions"
  },
  {
    "objectID": "how_much_data.html",
    "href": "how_much_data.html",
    "title": "How much data does an Autoencoder really need?",
    "section": "",
    "text": "Data requirements for generating useful representations\n\nWorking with the autoencoder we defined before in previous notebook"
  },
  {
    "objectID": "how_much_data.html#dataset",
    "href": "how_much_data.html#dataset",
    "title": "How much data does an Autoencoder really need?",
    "section": "Dataset",
    "text": "Dataset\n\nOur dataset is MNIST\n\nA selection of ~70,000 handwritten digits – a few sampels can be seen below:\n\n\n\n\nCode\nplot_dataset(train_df, num_samples=3)"
  },
  {
    "objectID": "how_much_data.html#model",
    "href": "how_much_data.html#model",
    "title": "How much data does an Autoencoder really need?",
    "section": "Model",
    "text": "Model\n\nWe’ll be using an incredibly simple model for this task\n\nBoth the encoder & decoder feature have the same structure (encoder shown below)\n\nThree convolutional layers with a single linear layer (660 MB RAM TOTAL) \n\n\n\n\n\nCode\nlearner.model.encoder\n\n\nSequential(\n  (0): Conv2d(1, 28, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (1): ReLU()\n  (2): Conv2d(28, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (3): ReLU()\n  (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (5): ReLU()\n  (6): Flatten(start_dim=1, end_dim=-1)\n  (7): Linear(in_features=2048, out_features=2, bias=True)\n)\n\n\n\nQuick training loop\n\nThis takes &lt;1 minute with a single GPU vs 4 min on macbook\n\n\n\nCode\nlearner.fine_tune(10, 3e-2)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n3402.974121\n3215.796387\n00:10\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n3089.285645\n3029.437256\n00:08\n\n\n1\n3047.984863\n3021.844482\n00:08\n\n\n2\n2986.998291\n2961.164551\n00:08\n\n\n3\n2961.992920\n2928.680908\n00:08\n\n\n4\n2862.808594\n2881.205078\n00:08\n\n\n5\n2832.602295\n2844.041992\n00:09\n\n\n6\n2797.664551\n2797.941650\n00:09\n\n\n7\n2742.184326\n2713.693604\n00:08\n\n\n8\n2705.889893\n2670.089844\n00:08\n\n\n9\n2638.182129\n2655.055908\n00:08"
  },
  {
    "objectID": "how_much_data.html#reconstructed-images",
    "href": "how_much_data.html#reconstructed-images",
    "title": "How much data does an Autoencoder really need?",
    "section": "Reconstructed Images",
    "text": "Reconstructed Images"
  },
  {
    "objectID": "how_much_data.html#latent-space-exploration",
    "href": "how_much_data.html#latent-space-exploration",
    "title": "How much data does an Autoencoder really need?",
    "section": "Latent space exploration",
    "text": "Latent space exploration\n\nLets see how our validation set gets mapped into embedding space via our encoder\n\nthe model is able to map each number into it’s own subspace – some overlap considerably more than others\n\n\n\n\nCode\nplot_latent_regen(learner.model, min=-100, max=100)\n\n\n\n\n\nleft: embedding scatter plot  right: reconstructed latents\n\n\n\nFigure 2: Embedding Space and Latent Reconstructions"
  },
  {
    "objectID": "autoencoder.html",
    "href": "autoencoder.html",
    "title": "Autoencoders!",
    "section": "",
    "text": "Can we use a generative framework in order to assist us in our process of designing new circuits?\n\nIn this notebook we’ll play with autoencoders to get a better feel for how they can work"
  },
  {
    "objectID": "autoencoder.html#dataset",
    "href": "autoencoder.html#dataset",
    "title": "Autoencoders!",
    "section": "Dataset",
    "text": "Dataset\n\nOur dataset is MNIST\n\nA selection of ~70,000 handwritten digits – a few sampels can be seen below:\n\n\n\n\nCode\nplot_dataset(train_df, num_samples=3)"
  },
  {
    "objectID": "autoencoder.html#model",
    "href": "autoencoder.html#model",
    "title": "Autoencoders!",
    "section": "Model",
    "text": "Model\n\nWe’ll be using an incredibly simple model for this task\n\nBoth the encoder & decoder feature have the same structure (encoder shown below)\n\nThree convolutional layers with a single linear layer (660 MB RAM TOTAL) \n\n\n\n\n\nCode\nlearner.model.encoder\n\n\nEncoder(\n  (conv_body): Sequential(\n    (0): Sequential(\n      (0): Conv2d(1, 28, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (1): ReLU()\n      (2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): Sequential(\n      (0): Conv2d(28, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (1): ReLU()\n      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): Sequential(\n      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (1): ReLU()\n      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (3): Flatten(start_dim=1, end_dim=-1)\n  )\n)\n\n\n\nQuick training loop\n\nThis takes &lt;1 minute with a single GPU vs 4 min on macbook\n\n\n\nCode\nlearner.fine_tune(12, 3e-2)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n5155.662598\n4945.959473\n00:11\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n4468.581543\n4515.016602\n00:10\n\n\n1\n3757.476807\n3749.431152\n00:10\n\n\n2\n2952.762695\n2943.328125\n00:10\n\n\n3\n2412.282959\n2392.489746\n00:10\n\n\n4\n2201.123535\n2181.014648\n00:10\n\n\n5\n2136.700684\n2128.168701\n00:10\n\n\n6\n2079.706299\n2101.125488\n00:10\n\n\n7\n2062.214600\n2058.276123\n00:10\n\n\n8\n2040.573120\n2023.409058\n00:10\n\n\n9\n1962.690796\n1995.426025\n00:10\n\n\n10\n1921.806641\n1983.867798\n00:10\n\n\n11\n1950.791992\n1979.757446\n00:10"
  },
  {
    "objectID": "autoencoder.html#reconstructed-images",
    "href": "autoencoder.html#reconstructed-images",
    "title": "Autoencoders!",
    "section": "Reconstructed Images",
    "text": "Reconstructed Images"
  },
  {
    "objectID": "autoencoder.html#latent-space-exploration",
    "href": "autoencoder.html#latent-space-exploration",
    "title": "Autoencoders!",
    "section": "Latent space exploration",
    "text": "Latent space exploration\n\nLets see how our validation set gets mapped into embedding space via our encoder\n\nthe model is able to map each number into it’s own subspace – some overlap considerably more than others\n\n\n\n\nCode\nplot_2d_latent_regen(learner.model, min=-200, max=200)\n\n\n\n\n\nleft: embedding scatter plot  right: reconstructed latents\n\n\n\nFigure 2: Embedding Space and Latent Reconstructions"
  },
  {
    "objectID": "init_autoencoder.html",
    "href": "init_autoencoder.html",
    "title": "Some plotting functionality to help with showing our original vs generated images",
    "section": "",
    "text": "Code\nfrom data import *\nimport matplotlib.pyplot as plt\n\n\n\n\nCode\ntrain_df = get_mnist_df()\nvalid_df = get_mnist_df(train=False)\ntrain_df.shape, valid_df.shape\n\n\n((60000, 2), (10000, 2))\n\n\n\n\nCode\ntrain_df.head(2)\n\n\n\n\n\n\n\n\n\npath\nlabel\n\n\n\n\n0\ndata/mnist_png/training/9/6813.png\n9\n\n\n1\ndata/mnist_png/training/9/6838.png\n9\n\n\n\n\n\n\n\n\n\nCode\nimg = get_img(train_df.path.iloc[0])\nimg\n\n\n\n\n\n\n\nCode\ntrain_dl, valid_dl = get_dls(train_df, valid_df, bs=64)\nbatch = next(iter(train_dl))\nbatch[0].shape\n\n\ntorch.Size([64, 1, 28, 28])\n\n\n\n\nCode\nb1 = batch[0][:8]\nb2 = batch[0][8:16]\nb1.shape, b2.shape\n\n\n(torch.Size([8, 1, 28, 28]), torch.Size([8, 1, 28, 28]))\n\n\n\n\nCode\ndef get_subplot(img, row=1, cols=2, item=1):\n    ax = plt.subplot(row, cols, item)\n    ax.imshow(np.array(img.squeeze()))\n    ax.axis('off')\n    return ax\n\ndef plt_subs(og_imgs, gen_imgs, size=3):\n    rows = int(len(og_imgs) / 2)\n    cols = 4\n    fig = plt.figure(figsize=(size*2, size*rows))\n\n    for idx, imgs in enumerate(zip(og_imgs, gen_imgs)):\n        ax1 = get_subplot(imgs[0], row=rows, cols=cols, item=2*idx+1)\n        ax2 = get_subplot(imgs[1], row=rows, cols=cols, item=2*idx+2)\n        if idx == 0 or idx == 1:\n            ax1.set_title(\"Original Images\", fontdict={'fontsize':7})\n            ax2.set_title(\"Generated Images\", fontdict={'fontsize':7})\n    plt.tight_layout(w_pad=0.5, h_pad=0.5)\n    fig.subplots_adjust(top=0.5)\n\n\n\n\nCode\nplt_subs(b1, b2, size=2)\n\n\n\n\n\n\nWe now have plotting functionality to allow us to compare original images to their generated counterparts\n\n\nCode\ndef get_encoder():\n    return torch.nn.Sequential(torch.nn.Conv2d(1, 28, 3, 2, 1),\n                              torch.nn.ReLU(),\n                              torch.nn.Conv2d(28, 64, 3, 2, 1),\n                              torch.nn.ReLU(),\n                              torch.nn.Conv2d(64, 128, 3, 2, 1),\n                              torch.nn.ReLU(),\n                              torch.nn.Flatten(),\n                              torch.nn.Linear(2048, 2)\n                             )\n\n\n\n\nCode\ndef get_decoder():\n    return torch.nn.Sequential(torch.nn.Linear(2, 2048),\n                                torch.nn.ReLU(),\n\n                                # Unflatten\n                                torch.nn.Unflatten(1, (128, 4, 4)),\n\n                                # Upsample\n                                torch.nn.ConvTranspose2d(128, 64, 3, 2, 1,),\n                                torch.nn.ReLU(),\n                                torch.nn.ConvTranspose2d(64, 28, 4, 2, 1,),\n                                torch.nn.ReLU(),\n                                torch.nn.ConvTranspose2d(28, 1, 4, 2, 1,)  # Use a kernel size of 4 here\n                            )\n\n\n\n\nCode\nclass Autoencoder(nn.Module):\n    def __init__(self):\n        super(Autoencoder, self).__init__()\n        self.encoder = get_encoder()\n        self.decoder = get_decoder()\n    \n    def forward(self, x):\n#        pdb.set_trace()\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x\n    \n    def encode(self, x):\n        x = self.encoder(x)\n        return x\n    \n    def decode(self, x):\n        x = self.decoder(x)\n        return x\n\n\n\n\nCode\ndls = DataLoaders(train_dl, valid_dl)\ndls = dls.to('cuda')\n\n\n\n\nCode\nlearner = Learner(dls, Autoencoder(), loss_func=MSELossFlat(), ).to_fp16()\nlearner.fine_tune(2, 3e-2)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n3414.484375\n3243.834717\n00:08\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n3024.529541\n3018.455322\n00:08\n\n\n1\n2827.781982\n2817.714111\n00:08\n\n\n\n\n\n\n\nCode\ndef get_valid_results(model, dl, num_samples=4):\n    batch = next(iter(dl))\n    output = model(batch[0].cuda()).detach().cpu()\n    return batch[0][:num_samples].cpu(), output[:num_samples]\n\n\n\n\nCode\nog_imgs, gen_imgs = get_valid_results(learner.model, dls.train, num_samples=8)\nplt_subs(og_imgs, gen_imgs, size=2)\n\n\n\n\n\n\n\nCode\ndef get_embed_plot(model):\n    test_ds = LabeledDataset(valid_df)\n    test_dl = DataLoader(test_ds, batch_size=512, shuffle=False)\n    model.to('cpu');\n    test_embeds = [model.encode(batch[0]) for batch in test_dl]\n    embeds = torch.vstack(test_embeds[:-1]).detach().numpy()\n    lbls = [batch[1] for batch in test_dl]\n    lbls = np.hstack([np.array(x).astype('int') for x in lbls[:-1]])\n    plt.scatter(embeds[:, 0], embeds[:, 1], c=lbls, s=3, cmap='rainbow')\n    plt.plot(2,20, 'ro')\n    plt.colorbar()\n    plt.show()\n\n\n\n\nCode\nget_embed_plot(learner.model)\n\n\n\n\n\n\n\nCode\ndef plot_regen(model, x=2, y=20):\n    regen = model.decode(torch.tensor([x,y]).float().unsqueeze(0))\n    plt.imshow(regen.squeeze().detach().numpy())\n\n\n\n\nCode\nplot_regen(learner.model, x=20, y=10)\n\n\n\n\n\n\n\nCode\nlearner = Learner(dls, Autoencoder(), loss_func=MSELossFlat(), ).to_fp16()\nlearner.fine_tune(10, 3e-1)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n4457.381348\n4462.031250\n00:09\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n4380.881348\n4396.578613\n00:09\n\n\n1\n4369.358887\n4403.636230\n00:09\n\n\n2\n4422.305176\n4447.104980\n00:09\n\n\n3\n4402.302734\n4405.910645\n00:09\n\n\n4\n4360.163574\n4420.194336\n00:09\n\n\n5\n4391.961914\n4395.602051\n00:08\n\n\n6\n4396.675781\n4394.964355\n00:09\n\n\n7\n4373.316895\n4391.890137\n00:09\n\n\n8\n4367.094727\n4388.964844\n00:09\n\n\n9\n4346.027832\n4389.003418\n00:09\n\n\n\n\n\n\n\nCode\noutput = learner.model(batch[0].cuda()).detach().cpu()\nplt_subs(batch[0][:4].cpu(), output[:4])\n\n\n\n\n\n\n\nCode\nget_embed_plot(learner.model)\n\n\n\n\n\n\n\nCode\nget_valid_resultsr = Learner(dls, Autoencoder(), loss_func=MSELossFlat(), ).to_fp16()\nlearner.fine_tune(20, 3e-3)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n3263.639893\n3181.919922\n00:08\n\n\n\n\n\n\n\n\n\n\n    \n      \n      90.00% [18/20 02:42&lt;00:18]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n3007.714844\n2993.816406\n00:08\n\n\n1\n2960.449951\n2933.150879\n00:09\n\n\n2\n2932.466553\n2917.204346\n00:09\n\n\n3\n2872.346680\n2869.713623\n00:09\n\n\n4\n2886.185303\n2903.034912\n00:09\n\n\n5\n2845.536621\n2840.021973\n00:09\n\n\n6\n2816.304199\n2816.199707\n00:09\n\n\n7\n2750.739990\n2751.230225\n00:08\n\n\n8\n2734.307373\n2715.794678\n00:08\n\n\n9\n2691.731689\n2704.523438\n00:09\n\n\n10\n2639.899170\n2663.206543\n00:08\n\n\n11\n2645.682129\n2638.991699\n00:09\n\n\n12\n2599.475586\n2617.349609\n00:08\n\n\n13\n2573.642578\n2601.716309\n00:09\n\n\n14\n2562.878418\n2578.189697\n00:09\n\n\n15\n2545.103760\n2561.594238\n00:08\n\n\n16\n2529.627441\n2553.733398\n00:09\n\n\n17\n2485.838135\n2545.210693\n00:09\n\n\n\n\n\n    \n      \n      67.70% [635/938 00:05&lt;00:02 2488.5779]\n    \n    \n\n\nKeyboardInterrupt: \n\n\n\n\nCode\noutput = learner.model(batch[0].cuda()).detach().cpu()\nplt_subs(batch[0][:4].cpu(), output[:4])\n\n\n\n\nCode\nget_embed_plot(learner.model)\n\n\nSo we have dataloaders that can give us our images.\nWe now need an abstraction model class for both encoder and decoder and lets create our first AutoEncoder!\n\n\nCode\nembeds.mean(axis=0)\n\n\n\n\nCode\nlearner.model.decode"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploring the world of generative modeling",
    "section": "",
    "text": "First we’ll start off with a something simple Autoencoder\nWe’ll then we’ll make it even more robust: Variational Autoencoders\n\nA brief foray to better understand the dataset requirements – How much data do we need?!\n\n\n This was built with Quarto `"
  }
]