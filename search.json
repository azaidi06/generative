[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Random forays into generative modeling. Starting with basic models/items like autoencoders and slowly making the way up to more complex things like stable diffusion(?)\nFor the most part everything will be done with Pytorch/FastAI"
  },
  {
    "objectID": "init_autoencoder.html",
    "href": "init_autoencoder.html",
    "title": "Some plotting functionality to help with showing our original vs generated images",
    "section": "",
    "text": "Code\nfrom data import *\nimport matplotlib.pyplot as plt\n\n\n\n\nCode\ntrain_df = get_mnist_df()\nvalid_df = get_mnist_df(train=False)\ntrain_df.shape, valid_df.shape\n\n\n((60000, 2), (10000, 2))\n\n\n\n\nCode\ntrain_df.head(2)\n\n\n\n\n\n\n\n\n\npath\nlabel\n\n\n\n\n0\ndata/mnist_png/training/9/6813.png\n9\n\n\n1\ndata/mnist_png/training/9/6838.png\n9\n\n\n\n\n\n\n\n\n\nCode\nimg = get_img(train_df.path.iloc[0])\nimg\n\n\n\n\n\n\n\nCode\ntrain_dl, valid_dl = get_dls(train_df, valid_df, bs=64)\nbatch = next(iter(train_dl))\nbatch[0].shape\n\n\ntorch.Size([64, 1, 28, 28])\n\n\n\n\nCode\nb1 = batch[0][:8]\nb2 = batch[0][8:16]\nb1.shape, b2.shape\n\n\n(torch.Size([8, 1, 28, 28]), torch.Size([8, 1, 28, 28]))\n\n\n\n\nCode\ndef get_subplot(img, row=1, cols=2, item=1):\n    ax = plt.subplot(row, cols, item)\n    ax.imshow(np.array(img.squeeze()))\n    ax.axis('off')\n    return ax\n\ndef plt_subs(og_imgs, gen_imgs, size=3):\n    rows = int(len(og_imgs) / 2)\n    cols = 4\n    fig = plt.figure(figsize=(size*2, size*rows))\n\n    for idx, imgs in enumerate(zip(og_imgs, gen_imgs)):\n        ax1 = get_subplot(imgs[0], row=rows, cols=cols, item=2*idx+1)\n        ax2 = get_subplot(imgs[1], row=rows, cols=cols, item=2*idx+2)\n        if idx == 0 or idx == 1:\n            ax1.set_title(\"Original Images\", fontdict={'fontsize':7})\n            ax2.set_title(\"Generated Images\", fontdict={'fontsize':7})\n    plt.tight_layout(w_pad=0.5, h_pad=0.5)\n    fig.subplots_adjust(top=0.5)\n\n\n\n\nCode\nplt_subs(b1, b2, size=2)\n\n\n\n\n\n\nWe now have plotting functionality to allow us to compare original images to their generated counterparts\n\n\nCode\ndef get_encoder():\n    return torch.nn.Sequential(torch.nn.Conv2d(1, 28, 3, 2, 1),\n                              torch.nn.ReLU(),\n                              torch.nn.Conv2d(28, 64, 3, 2, 1),\n                              torch.nn.ReLU(),\n                              torch.nn.Conv2d(64, 128, 3, 2, 1),\n                              torch.nn.ReLU(),\n                              torch.nn.Flatten(),\n                              torch.nn.Linear(2048, 2)\n                             )\n\n\n\n\nCode\ndef get_decoder():\n    return torch.nn.Sequential(torch.nn.Linear(2, 2048),\n                                torch.nn.ReLU(),\n\n                                # Unflatten\n                                torch.nn.Unflatten(1, (128, 4, 4)),\n\n                                # Upsample\n                                torch.nn.ConvTranspose2d(128, 64, 3, 2, 1,),\n                                torch.nn.ReLU(),\n                                torch.nn.ConvTranspose2d(64, 28, 4, 2, 1,),\n                                torch.nn.ReLU(),\n                                torch.nn.ConvTranspose2d(28, 1, 4, 2, 1,)  # Use a kernel size of 4 here\n                            )\n\n\n\n\nCode\nclass Autoencoder(nn.Module):\n    def __init__(self):\n        super(Autoencoder, self).__init__()\n        self.encoder = get_encoder()\n        self.decoder = get_decoder()\n    \n    def forward(self, x):\n#        pdb.set_trace()\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x\n    \n    def encode(self, x):\n        x = self.encoder(x)\n        return x\n    \n    def decode(self, x):\n        x = self.decoder(x)\n        return x\n\n\n\n\nCode\ndls = DataLoaders(train_dl, valid_dl)\ndls = dls.to('cuda')\n\n\n\n\nCode\nlearner = Learner(dls, Autoencoder(), loss_func=MSELossFlat(), ).to_fp16()\nlearner.fine_tune(2, 3e-2)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n3414.484375\n3243.834717\n00:08\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n3024.529541\n3018.455322\n00:08\n\n\n1\n2827.781982\n2817.714111\n00:08\n\n\n\n\n\n\n\nCode\ndef get_valid_results(model, dl, num_samples=4):\n    batch = next(iter(dl))\n    output = model(batch[0].cuda()).detach().cpu()\n    return batch[0][:num_samples].cpu(), output[:num_samples]\n\n\n\n\nCode\nog_imgs, gen_imgs = get_valid_results(learner.model, dls.train, num_samples=8)\nplt_subs(og_imgs, gen_imgs, size=2)\n\n\n\n\n\n\n\nCode\ndef get_embed_plot(model):\n    test_ds = LabeledDataset(valid_df)\n    test_dl = DataLoader(test_ds, batch_size=512, shuffle=False)\n    model.to('cpu');\n    test_embeds = [model.encode(batch[0]) for batch in test_dl]\n    embeds = torch.vstack(test_embeds[:-1]).detach().numpy()\n    lbls = [batch[1] for batch in test_dl]\n    lbls = np.hstack([np.array(x).astype('int') for x in lbls[:-1]])\n    plt.scatter(embeds[:, 0], embeds[:, 1], c=lbls, s=3, cmap='rainbow')\n    plt.plot(2,20, 'ro')\n    plt.colorbar()\n    plt.show()\n\n\n\n\nCode\nget_embed_plot(learner.model)\n\n\n\n\n\n\n\nCode\ndef plot_regen(model, x=2, y=20):\n    regen = model.decode(torch.tensor([x,y]).float().unsqueeze(0))\n    plt.imshow(regen.squeeze().detach().numpy())\n\n\n\n\nCode\nplot_regen(learner.model, x=20, y=10)\n\n\n\n\n\n\n\nCode\nlearner = Learner(dls, Autoencoder(), loss_func=MSELossFlat(), ).to_fp16()\nlearner.fine_tune(10, 3e-1)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n4457.381348\n4462.031250\n00:09\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n4380.881348\n4396.578613\n00:09\n\n\n1\n4369.358887\n4403.636230\n00:09\n\n\n2\n4422.305176\n4447.104980\n00:09\n\n\n3\n4402.302734\n4405.910645\n00:09\n\n\n4\n4360.163574\n4420.194336\n00:09\n\n\n5\n4391.961914\n4395.602051\n00:08\n\n\n6\n4396.675781\n4394.964355\n00:09\n\n\n7\n4373.316895\n4391.890137\n00:09\n\n\n8\n4367.094727\n4388.964844\n00:09\n\n\n9\n4346.027832\n4389.003418\n00:09\n\n\n\n\n\n\n\nCode\noutput = learner.model(batch[0].cuda()).detach().cpu()\nplt_subs(batch[0][:4].cpu(), output[:4])\n\n\n\n\n\n\n\nCode\nget_embed_plot(learner.model)\n\n\n\n\n\n\n\nCode\nget_valid_resultsr = Learner(dls, Autoencoder(), loss_func=MSELossFlat(), ).to_fp16()\nlearner.fine_tune(20, 3e-3)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n3263.639893\n3181.919922\n00:08\n\n\n\n\n\n\n\n\n\n\n    \n      \n      90.00% [18/20 02:42&lt;00:18]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n3007.714844\n2993.816406\n00:08\n\n\n1\n2960.449951\n2933.150879\n00:09\n\n\n2\n2932.466553\n2917.204346\n00:09\n\n\n3\n2872.346680\n2869.713623\n00:09\n\n\n4\n2886.185303\n2903.034912\n00:09\n\n\n5\n2845.536621\n2840.021973\n00:09\n\n\n6\n2816.304199\n2816.199707\n00:09\n\n\n7\n2750.739990\n2751.230225\n00:08\n\n\n8\n2734.307373\n2715.794678\n00:08\n\n\n9\n2691.731689\n2704.523438\n00:09\n\n\n10\n2639.899170\n2663.206543\n00:08\n\n\n11\n2645.682129\n2638.991699\n00:09\n\n\n12\n2599.475586\n2617.349609\n00:08\n\n\n13\n2573.642578\n2601.716309\n00:09\n\n\n14\n2562.878418\n2578.189697\n00:09\n\n\n15\n2545.103760\n2561.594238\n00:08\n\n\n16\n2529.627441\n2553.733398\n00:09\n\n\n17\n2485.838135\n2545.210693\n00:09\n\n\n\n\n\n    \n      \n      67.70% [635/938 00:05&lt;00:02 2488.5779]\n    \n    \n\n\nKeyboardInterrupt: \n\n\n\n\nCode\noutput = learner.model(batch[0].cuda()).detach().cpu()\nplt_subs(batch[0][:4].cpu(), output[:4])\n\n\n\n\nCode\nget_embed_plot(learner.model)\n\n\nSo we have dataloaders that can give us our images.\nWe now need an abstraction model class for both encoder and decoder and lets create our first AutoEncoder!\n\n\nCode\nembeds.mean(axis=0)\n\n\n\n\nCode\nlearner.model.decode"
  },
  {
    "objectID": "autoencoder.html",
    "href": "autoencoder.html",
    "title": "Autoencoders!",
    "section": "",
    "text": "Can we use a generative framework in order to assist us in our process of designing new circuits?\n\nIn this notebook we’ll play with autoencoders to get a better feel for how they can work"
  },
  {
    "objectID": "autoencoder.html#dataset",
    "href": "autoencoder.html#dataset",
    "title": "Autoencoders!",
    "section": "Dataset",
    "text": "Dataset\n\nOur dataset is MNIST\n\nA selection of ~70,000 handwritten digits – a few sampels can be seen below:\n\n\n\n\nCode\nplot_dataset(train_df, num_samples=3)"
  },
  {
    "objectID": "autoencoder.html#model",
    "href": "autoencoder.html#model",
    "title": "Autoencoders!",
    "section": "Model",
    "text": "Model\n\nWe’ll be using an incredibly simple model for this task\n\nBoth the encoder & decoder feature have the same structure (encoder shown below)\n\nThree convolutional layers with a single linear layer (660 MB RAM TOTAL) \n\n\n\n\n\nCode\nlearner.model.encoder\n\n\nSequential(\n  (0): Conv2d(1, 28, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (1): ReLU()\n  (2): Conv2d(28, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (3): ReLU()\n  (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (5): ReLU()\n  (6): Flatten(start_dim=1, end_dim=-1)\n  (7): Linear(in_features=2048, out_features=2, bias=True)\n)\n\n\n\nQuick training loop\n\nThis takes &lt;1 minute with a single GPU vs 4 min on macbook\n\n\n\nCode\nlearner.fine_tune(3, 3e-2)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n3331.510742\n3205.732666\n00:59\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n3113.601562\n3133.075684\n00:59\n\n\n1\n2983.510010\n2938.452881\n00:59\n\n\n2\n2805.703369\n2815.310547\n00:59"
  },
  {
    "objectID": "autoencoder.html#reconstructed-images",
    "href": "autoencoder.html#reconstructed-images",
    "title": "Autoencoders!",
    "section": "Reconstructed Images",
    "text": "Reconstructed Images"
  },
  {
    "objectID": "autoencoder.html#latent-space-exploration",
    "href": "autoencoder.html#latent-space-exploration",
    "title": "Autoencoders!",
    "section": "Latent space exploration",
    "text": "Latent space exploration\n\nLets see how our validation set gets mapped into embedding space via our encoder\n\nthe model is able to map each number into it’s own subspace – some overlap considerably more than others\n\n\n\n\nCode\nplot_latent_regen(learner.model, min=-100, max=100)\n\n\n\n\n\nleft: embedding scatter plot  right: reconstructed latents\n\n\n\nFigure 2: Embedding Space and Latent Reconstructions"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploring the world of generative modeling",
    "section": "",
    "text": "First we’ll start off with a something simple Autoencoder\n\nAn exploration on dataset requirements – How much data do we need?!\n\nWe’ll then we’ll make it even more robust: Variational Autoencoders\nCan GANs take us to the next level?\n\n The associated code can be found here: https://github.com/azaidi06/generative This website was built with Quarto!"
  }
]